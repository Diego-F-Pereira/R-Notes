Test - read.csv(). Faster and more efficient with nrows?
========================================================
As everybody knows when dealing with large files is important to know the actual number of lines or rows in a data set for preallocating purposes so memory handle in R can be improved.  

But as usually happens whith those things everybody knows, they are not always true.  

Here I show the results of my own tests which doesn't seem to support that claim.

Counting the number of lines in a file can be handled from outside R by using the **shell()** function of the **base** package or by using R functions directly.   

Usual file formats do not represent a problem for using the **shell** technique; however many times we need to deal with messy files and/or with compressed files for which the count of lines can be innacurate.

In any case let's see how different approaches perform with compressed and uncompressed files.

First prepare our environment:
```{r}
rm(list=ls())
gc()
my.path <- "C:/Users/Diego/Documents/Reproducible Research/PA2"
setwd(my.path)
fileZip <- "./StormData.csv.bz2"
fileUnz <- "./StormData.csv"
```
### Extract the zipped file
```{r}
library(R.utils)
if(file.exists(fileUnz)==F){
  system.time(bunzip2(fileZip))
}
```

Using R functions
--------------------------------------------------------
#### Zipped countLines using a connection
```{r}
con <- file(fileZip, "rb")
system.time(nrec01 <- countLines(con))
close(con)
nrec01
```
#### Zipped countLines using directly the file
```{r}
system.time(nrec01 <- countLines(fileZip))
nrec01
```
```{r hk1,echo=FALSE,results='hide'}
rm(nrec01)
gc()
```

#### Unzipped countLines
```{r}
system.time(nrec02 <- countLines(fileUnz))
nrec02
```
```{r hk2,echo=FALSE,results='hide'}
rm(nrec02)
gc()
```
### Zipped scan
```{r}
system.time(nrec03 <- length(scan(file = fileZip, what = list(""), n = -1, sep = "\n")[[1]]))
nrec03
```
```{r hk3,echo=FALSE,results='hide'}
rm(nrec03)
gc()
```
### Unzipped scan
```{r}
system.time(nrec04 <- length(scan(file = fileUnz, what = list(""), n = -1, sep = "\n")[[1]]))
nrec04
```
```{r hk4,echo=FALSE,results='hide'}
rm(nrec04)
gc()
```
### fread
```{r}
library(data.table)
a <- fread(fileUnz)
```
```{r hk5,echo=FALSE,results='hide'}
rm(a)
gc()
```
### Zipped hsTableReader
```{r}
nrec05 <- 0
nlines <-function(d) {
  nrec05 <<- nrec05 + nrow(d)
}
library(HadoopStreaming)
con <- bzfile(fileZip, "rb")
system.time(d <- hsTableReader(con, FUN=nlines))
close(con)
nrec05
```
```{r hk6,echo=FALSE,results='hide'}
rm(nrec05)
gc()
```
### Unzipped hsTableReader
```{r}
nrec06 <- 0
nlines <-function(d) {
  nrec06 <<- nrec06 + nrow(d)
}
con <- file(fileUnz,open="r")
system.time(d <- hsTableReader(con, FUN=nlines))
close(con)
nrec06
```
```{r hk7,echo=FALSE,results='hide'}
rm(nrec06)
gc()
```
### Zipped hsLineReader
```{r}
nrec07 <- 0
nlines <-function(d) {
  nrec07 <<- nrec07 + length(d)
}
con <- bzfile(fileZip, "rb")
system.time(d <- hsLineReader(con, FUN=nlines))
close(con)
nrec07
```
```{r hk8,echo=FALSE,results='hide'}
rm(nrec07)
gc()
```
### Unzipped hsLineReader
```{r}
nrec08 <- 0
nlines <-function(d) {
  nrec08 <<- nrec08 + length(d)
}
library(HadoopStreaming)
con <- file(fileUnz,open="r")
system.time(d <- hsLineReader(con, FUN=nlines))
close(con)
nrec08
```
```{r hk9,echo=FALSE,results='hide'}
rm(nrec08)
gc()
```
Using the Shell (Windows)
--------------------------------------------------------
### Unzipped find /c
```{r}
system.time(nrec09 <- as.numeric(shell('type "StormData.csv" | find /c ","', intern=T)))
nrec09
```
```{r hk10,echo=FALSE,results='hide'}
rm(nrec09)
gc()
```
### Unzipped find /v /c
```{r}
system.time(nrec10 <- as.numeric(shell('type "StormData.csv" | find /v /c ""', intern=T)))
nrec10
```
```{r hk11,echo=FALSE,results='hide'}
rm(nrec10)
gc()
```
### Zipped find /c
```{r}
system.time(nrec11 <- as.numeric(shell('type "StormData.csv.bz2" | find /c ","', intern=T)))
nrec11
```
```{r hk12,echo=FALSE,results='hide'}
rm(nrec11)
gc()
```
### Zipped find /v /c
```{r}
system.time(nrec12 <- as.numeric(shell('type "StormData.csv.bz2" | find /v /c ""', intern=T)))
nrec12
```
```{r hk13,echo=FALSE,results='hide'}
rm(nrec12)
gc()
```

First we notice is the variety of results returned by each approach.   

Importantly only the **hsTableReader()** approach returned the correct results for this messy data set, even when an *out of bounds* error jumps.   

Among other things this is due to the use of a .csv file for storing complex things.

Interestingly, is faster to attack directly the file than using a connection.   

Noticeably, is faster to use zipped files than unzipped files with some methods but not with others.

Finally, is evident that the **shell** approach is way more efficient than any of the tested internal R functions.

Unfortunately, fred crashes with this complex csv file.

Among all, the fastest approach was **Zipped find /v /c**, but the results are not right.

However, that result is very close to the half of the **Unzipped find /c** approach which is the one reported by Phil Spector in his book *"Data Manipulation with R"* which is my gold standard to call it some way.

I'm not sure if the naive multiplication by 2 can be generalizable to all cases, but since it is very fast one can test it whenever needed.

Comparison
--------------------------------------------------------
### Get the classes
```{r}
storm.data <- read.csv(fileZip, stringsAsFactors = F, nrows = 10)
classes <- sapply(storm.data, class)
```
```{r hk14,echo=FALSE,results='hide'}
rm(storm.data)
gc()
```
**read.csv()** loves conversions, and in this case getting the classes of the first rows doesn't help for passing the actual classes the colClasses argument as you can see below.
```{r Crash}
Sys.time()
system.time(storm.data <- read.csv(fileZip, 
                                   header = TRUE,
                                   stringsAsFactors = F,
                                   comment.char = "",
                                   colClasses = classes))
Sys.time()
```
```{r hk15,echo=FALSE,results='hide'}
gc()
```

### Without nrows
```{r !nrows}
Sys.time()
system.time(storm.data <- read.csv(fileZip, 
                                   header = TRUE,
                                   stringsAsFactors = F,
                                   comment.char = "",
                                   colClasses = "character"))
Sys.time()
```
```{r hk16,echo=FALSE,results='hide'}
rm(storm.data)
gc()
```
### With nrows
```{r nrows}
Sys.time()
system.time(nrec12 <- as.numeric(
  shell('type "StormData.csv.bz2" | find /v /c ""',
        intern=T)))
nrec12 <- nrec12 * 2
system.time(storm.data <- read.csv(fileZip, 
                                   stringsAsFactors = F,
                                   comment.char = "",
                                   colClasses = "character",
                                   nrows = nrec12))
Sys.time()
```
```{r hk17,echo=FALSE,results='hide'}
rm(storm.data)
gc()
```

While there's a little improvement in time is not that dramatic as I expected considering what others have reported.  

I mean, 12 seconds are valuable, but I spent ~2 days doing the tests.

I created an external application for tracking the memory usage by R during this process, and here are the results:
```{r RAMconsumption}
file1 <- "./Table1.txt"
file2 <- "./Table3.txt"

x <- read.csv(file1,
              stringsAsFactors = F,
              comment.char = "",
              colClasses = c("integer",
                             "character",
                             "integer"))
y <- read.csv(file2,
              stringsAsFactors = F,
              comment.char = "",
              colClasses = c("integer",
                             "character",
                             "integer"))

x$RAM <- as.integer(round(x$RAM * 0.000976562),0)
y$RAM <- as.integer(round(y$RAM * 0.000976562),0)

plot(x$ID,
     x$RAM,     
     type = "l",
      col = "blue",
     xlab = "Time (seconds)",
     ylab = "RAM Consumption (Megabytes)")
lines(y$RAM,
      type = "l",
       col = "red")
legend("topleft", legend = c("Without nrows", "With nrows"),
       col = c('blue','red'), lty = 1,lwd = 1.5) 
```
```{r hk18,echo=FALSE,results='hide'}
rm(list=ls())
gc()
```
Evidently, passing a number to the **nrows** argument allows **read.csv()** to preallocate the data frame for receiving the csv file.  

However, the RAM consumptions is even superior by preallocating!  

I'll need to do further tests with other data sets in oder to get a final conclusion, but at least in this case passing an argument to nrows doesn't seem to improve significatively the efficiency of the **read.csv()** function.

 ---
Note: For the test I had to close and re-start R, since as you can notice, even after calling the garbage collector the memory is not totally recovered.  
 ---

